{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9IJvl-EEESCA"
      },
      "source": [
        "# <font color=red>LangChain:  Example Generate Causal Reasoning Pre-training Data</font>\n",
        "- https://docs.langchain.com/docs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<span style=\"font-family:'Comic Sans MS', cursive, sans-serif;\"><font color=orange>\n",
        "## Demo 1 - Tiny example to generate data for a {subject}, e.g. biology\n",
        "</font></span>\n",
        "This program demos a way to generate causal reasoning examples that might be used to pre-train a model like AuroraGPT.</br>\n",
        "The program works with GPT-4 trained \"as-is\", i.e. it does not use \"trusted documents\" to make sure all is correct.</br>\n",
        "It also does not try to avoid hallucination with prompt directives such as \"don't make things up\".</br>\n",
        "<font color=lightgreen>Important note:</font></br>\n",
        "The program contains an exit(0) statement which you will need to comment out if you want to write the examples to a JSONL file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys, os, random, json\n",
        "\n",
        "from langchain import LLMChain, PromptTemplate\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "NUM_EXAMPLES_TO_GEN = 11\n",
        "\n",
        "# MODEL = \"gpt-3.5-turbo\"\n",
        "MODEL = \"gpt-4\"\n",
        "\n",
        "TEMPLATE = \"\"\"\n",
        "You are generating data which will be used to fine-tune a Large Language Model.\n",
        "The model will be used to work with advanced high-school students studying {subject}.\n",
        "You will be generating single prompt/response pairs which present examples of\n",
        "causal reasoning that advanced high-school students in {subject} can understand.\n",
        "There may also be some previously generated examples provided in the prompt to\n",
        "help you to ensure uniqueness and diversity.\n",
        "Please keep each response to a \"reasonable\" length, i.e. no more than 100 words.\n",
        "The prompt/response pair that you create should be in this format:\n",
        "Prompt: <prompt goes here>\n",
        "Response: <response goes here>\n",
        "\\n\n",
        "Question:\n",
        "{question}\n",
        "\"\"\"\n",
        "\n",
        "QUESTION = \"\"\"\n",
        "Please generate a single prompt/response pair which presents an example of\n",
        "causal reasoning that advanced high-school students in {subject} can understand.\n",
        "Be sure to avoid duplication with prior questions listed.\n",
        "Be sure to develop the response using a step-by-step process.\n",
        "\"\"\"\n",
        "\n",
        "subject = \"biology\"   # could be a cmd-line arg\n",
        "\n",
        "llm = ChatOpenAI(model_name=MODEL,temperature=0.5) # ,max_tokens=100)\n",
        "prompt_template = PromptTemplate.from_template(TEMPLATE)\n",
        "answer_chain = LLMChain(llm=llm, prompt=prompt_template)\n",
        "\n",
        "def generate_one_example(prev_examples):\n",
        "    if len(prev_examples) > 8:\n",
        "        prev_examples = random.sample(prev_examples,8)\n",
        "    prevs = \"\"\n",
        "    for example in prev_examples:\n",
        "        prevs += \"Previous example:\\n\" + example + \"\\n\"\n",
        "    question = prevs + QUESTION\n",
        "    answer = answer_chain.run(subject=subject,question=question)\n",
        "    return answer\n",
        "\n",
        "examples = []\n",
        "for i in range(NUM_EXAMPLES_TO_GEN):\n",
        "    print(f'Generating example {i}')\n",
        "    example = generate_one_example( examples )\n",
        "    examples.append(example)\n",
        "\n",
        "for (i,example) in enumerate(examples):\n",
        "    print(f\"\\nCausal Reasoning Example {i}\\n----------------------------\")\n",
        "    print(f\"{example}\\n\")\n",
        "\n",
        "system_message = \"\"\"\n",
        "Given a prompt which claims that a particular causal relationship exists in biology, generate a response which explains why that causal relationship does indeed exist.\n",
        "\"\"\"\n",
        "\n",
        "#### save the examples into a JSONL file\n",
        "## print(\"EXITING\") ; exit(0)  # un-comment this if you want to skip creating the JSONL file\n",
        "if False:   # change to True if you want to create the JSONL file\n",
        "\n",
        "    filename = f\"{subject}_training_data.jsonl\"\n",
        "    with open(filename, 'w') as jsonl_file:\n",
        "        for example in examples:\n",
        "            idx = example.find(\"Response:\")\n",
        "            sys_msg  = system_message.strip()\n",
        "            user_msg = example[:idx].strip()\n",
        "            asst_msg = example[idx:].strip()\n",
        "            user_msg = user_msg.replace(\"Prompt: \", \"\")\n",
        "            asst_msg = asst_msg.replace(\"Response: \", \"\")\n",
        "            training_example = {\n",
        "                \"messages\": [\n",
        "                    {\"role\": \"system\",    \"content\": sys_msg},\n",
        "                    {\"role\": \"user\",      \"content\": user_msg},\n",
        "                    {\"role\": \"assistant\", \"content\": asst_msg}\n",
        "                ]\n",
        "            }\n",
        "            print(json.dumps(training_example), file=jsonl_file)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyMZi6NDm9npKydMJRyJPAT2",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
