{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9IJvl-EEESCA"
      },
      "source": [
        "# <font color=red>LangChain:  Vector DBs</font>\n",
        "- https://docs.langchain.com/docs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## What Does LangChain Provide?\n",
        "+ Models\n",
        "  + embedding\n",
        "  + LLM (e.g. OpenAI)\n",
        "+ Prompts\n",
        "  + prompt templates\n",
        "  + few-shot\n",
        "  + example-selectors\n",
        "  + output parsers\n",
        "+ Chains (a multi-step workflow composed of <em>links</em>)</br>\n",
        "  + Links (one of: prompt, model, another chain)\n",
        "<span style=\"font-family:'Comic Sans MS', cursive, sans-serif;\"><font color=orange>\n",
        "+ Vector Database Access\n",
        "  + Document Loaders\n",
        "  + Text Splitting \n",
        "</font></span>\n",
        "+ Memories (to facilitate chatbots or other 'iterative' sorts of apps)\n",
        "+ Agents (loop over Thought, Act, Observe)\n",
        "  + Tools\n",
        "    + math\n",
        "    + web search\n",
        "    + custom (user-defined)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<span style=\"font-family:'Comic Sans MS', cursive, sans-serif;\"><font color=orange>\n",
        "## Vector Database Access\n",
        "</font></span>\n",
        "There are a lot of vector DBs available via LangChain.</br>\n",
        "Perhaps the most well-known commercial one is Pinecone.  It requires setup at their site.</br>\n",
        "We will be using a couple of free ones here:  FAISS and Chroma\n",
        "<span style=\"font-family:'Comic Sans MS', cursive, sans-serif;\"><font color=orange>\n",
        "### Document Loaders and Text Splitting\n",
        "</font></span>\n",
        "These examples are somewhat longer because they not only demo using vector DBs, but also use chains to demo</br>\n",
        "the VDBs being used to retrieve useful info. \n",
        "<font color=green>These examples read a set of *.txt and *.pdf files from sub-directories named txt and pdf,</br>\n",
        "which are supplied with this notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Demo loading txt files (no pdfs)\n",
        "This demo uses the <font color=green>Chroma</font> vector DB which is quite popular.</br> \n",
        "We also save (persist) the DB to disk.</br>\n",
        "We use a RetrievalQA chain to prove we can use the DB to retrieve document content and information."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install chromadb   ## you may have to do this if not already installed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.vectorstores import Chroma   ## use chroma vector DB\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.document_loaders import DirectoryLoader\n",
        "\n",
        "loader = DirectoryLoader('./', glob=\"./txt/*.txt\", loader_cls=TextLoader)\n",
        "# loader = TextLoader('./one_file.txt')\n",
        "\n",
        "documents = loader.load()\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
        "texts = text_splitter.split_documents(documents)\n",
        "\n",
        "embeddings = OpenAIEmbeddings()\n",
        "vectordb = Chroma.from_documents(documents=texts, embedding=embeddings, persist_directory=\"chroma_db\")\n",
        "\n",
        "## first, prove we can obtain the relevant docs\n",
        "retriever = vectordb.as_retriever()\n",
        "docs = retriever.get_relevant_documents( \"What did Abraham Lincoln say our fathers had brought forth on this continent?\" )\n",
        "print(\"RELEVANT DOCS\")\n",
        "print(docs)\n",
        "\n",
        "# k docs to return, default 4\n",
        "retriever = vectordb.as_retriever(search_kwargs={\"k\": 2})\n",
        "print(\"SEARCH TYPE\",retriever.search_type)\n",
        "print(\"SEARCH KWARGS\",retriever.search_kwargs)\n",
        "\n",
        "llm = ChatOpenAI(temperature=0.0, model_name='gpt-4')\n",
        "\n",
        "qa_chain = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=retriever, return_source_documents=True)\n",
        "\n",
        "# get the sources from the response\n",
        "def process_llm_response(llm_response):\n",
        "    print(llm_response['result'])\n",
        "    print('\\n\\nSources:')\n",
        "    for source in llm_response[\"source_documents\"]:\n",
        "        print(source.metadata['source'])\n",
        "\n",
        "# full example\n",
        "query = \"What did Abraham Lincoln say our fathers had brought forth on this continent?\"\n",
        "llm_response = qa_chain(query)\n",
        "print(\"LLM_RESPONSE\")\n",
        "print(llm_response)\n",
        "print(\"PROCESSED OUTPUT\")\n",
        "process_llm_response(llm_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Here is a simpler example with Chroma but using more high-level operations from LangChain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys, os\n",
        "\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.indexes import VectorstoreIndexCreator\n",
        "\n",
        "to_summarize = \"./txt/cuban.txt\"\n",
        "\n",
        "loaders = []\n",
        "for fn in os.listdir(\"./txt\"):\n",
        "    filename = \"./txt/\" + fn\n",
        "    loader = TextLoader(filename)\n",
        "    loaders.append(loader)\n",
        "index = VectorstoreIndexCreator().from_loaders(loaders)\n",
        "\n",
        "print(\"INDEX_VECTORSTORE\",index.vectorstore)\n",
        "print(\"AS_RETRIEVER\",index.vectorstore.as_retriever())\n",
        "print()\n",
        "\n",
        "query = \"What did Lincoln say that our fathers had brought forth on this continent?\"\n",
        "result = index.query(query)\n",
        "print(result,\"\\n\")\n",
        "\n",
        "query = \"What did Lincoln say that our fathers had brought forth on this continent?\"\n",
        "result = index.query_with_sources(query)\n",
        "# print(result,\"\\n\")\n",
        "print(result[\"answer\"])\n",
        "print(result[\"sources\"],\"\\n\")\n",
        "\n",
        "query = \"What happened on December 7, 1941?\"  # Dec. is abbreviated in the doc\n",
        "result = index.query_with_sources(query)\n",
        "# print(result,\"\\n\")\n",
        "print(result[\"answer\"])\n",
        "print(result[\"sources\"],\"\\n\")\n",
        "\n",
        "result = index.query(\"Summarize the general content of this document.\",\n",
        "                     retriever_kwargs={\"search_kwargs\": {\"filter\": {\"source\": to_summarize}}})\n",
        "print(result,\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Demo loading pdf files (not txt)\n",
        "This demo uses the <font color=green>FAISS</font> vector DB.</br> \n",
        "We use a regular LLMChain chain to prove we can use the DB to retrieve document content and information."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys, os, openai, textwrap\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.prompts.chat import (\n",
        "    ChatPromptTemplate,\n",
        "    SystemMessagePromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        ")\n",
        "\n",
        "\n",
        "pdf_filenames = [\"./pdf/\"+f for f in os.listdir('./pdf') if f.endswith(\".pdf\")]\n",
        "\n",
        "embeddings = OpenAIEmbeddings()\n",
        "all_pages = []\n",
        "for pdf_filename in pdf_filenames:\n",
        "    loader = PyPDFLoader(pdf_filename)\n",
        "    pages = loader.load_and_split()\n",
        "    all_pages.extend(pages)\n",
        "faiss_index = FAISS.from_documents(all_pages, embeddings)\n",
        "\n",
        "query = \"What is a generative agent?\"  ## NOTE: voyager_gpt4 pdf paper answers this\n",
        "\n",
        "# gpt-4 can handle up to 8192 tokens.  Set chunksize to 1000 and k to 8.\n",
        "docs = faiss_index.similarity_search(query, k=8)  # k=8 is default\n",
        "docs_page_content = \" \".join([d.page_content for d in docs])\n",
        "\n",
        "chat = ChatOpenAI(model_name=\"gpt-4\", temperature=0.1)\n",
        "\n",
        "system_msg_template = \"\"\"\n",
        "    You are a helpful assistant that that can answer questions about content\n",
        "    obtained from pdf documents: {docs}\n",
        "    Only use the factual information from the documents to answer the question.\n",
        "    If you don't have enough information to answer the question, simply say \"I don't know\".\n",
        "    Your answer should be concise but provide sufficient detail to fully answer.\n",
        "\"\"\"\n",
        "system_msg_prompt = SystemMessagePromptTemplate.from_template(system_msg_template)\n",
        "\n",
        "###### NOTE: the human_template determines whether you ask which document is relevant to the\n",
        "######       question, or if you ask for the actual answer to the question\n",
        "human_template = \"Which document provides the best answer to this question: {question}\"\n",
        "human_template = \"Answer the following question: {question}\"\n",
        "human_msg_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
        "chat_prompt = ChatPromptTemplate.from_messages( [system_msg_prompt,human_msg_prompt])\n",
        "\n",
        "chain = LLMChain(llm=chat,prompt=chat_prompt)\n",
        "response = chain.run(question=query, docs=docs_page_content,return_source_documents=True)\n",
        "response = response.replace(\"\\n\", \"\")\n",
        "print(f\"\\nanswer:\\n    {textwrap.fill(response, width=70)}\")\n",
        "\n",
        "print(\"\\nsource page info:\")\n",
        "for doc in docs:\n",
        "    print(f\"    {doc.metadata}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Contextual Compression With Documents Example \n",
        "From the LangChain documentation:</br>\n",
        "The idea is simple: instead of immediately returning retrieved documents as-is, you can compress them using the context of the given query, so that only the relevant information is returned. “Compressing” here refers to both compressing the contents of an individual document and filtering out documents wholesale."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain import LLMChain\n",
        "from langchain.retrievers import ContextualCompressionRetriever\n",
        "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
        "\n",
        "def pretty_print_docs(docs):\n",
        "    for (i,doc) in enumerate(docs):\n",
        "        print(\"-\"*70)\n",
        "        print(f\"Document {i+1}:\\n\")\n",
        "        print(doc.page_content)\n",
        "\n",
        "\n",
        "## first, get the relevant docs in the \"usual\" way\n",
        "\n",
        "loader = PyPDFLoader(\"./pdf/voyager_minecraft_gpt4.pdf\")\n",
        "pages = loader.load()\n",
        "text_splitter = CharacterTextSplitter(chunk_size=1000,chunk_overlap=0)\n",
        "texts = text_splitter.split_documents(pages)\n",
        "embeddings = OpenAIEmbeddings()\n",
        "retriever = FAISS.from_documents(texts,embeddings).as_retriever()\n",
        "\n",
        "docs = retriever.get_relevant_documents(\"What are generative agents?\")\n",
        "pretty_print_docs(docs)\n",
        "\n",
        "## then, get compressed relevant docs using compression retriever\n",
        "##   then go ahead and use the llm to answer the question\n",
        "\n",
        "template = \"\"\"\n",
        "You are a useful assistant.\n",
        "Please answer the question within the given context.\n",
        "Context: {context}\n",
        "Question: {question}\n",
        "\"\"\"\n",
        "\n",
        "prompt = PromptTemplate.from_template(template)\n",
        "\n",
        "MODEL_NAME = \"gpt-3.5-turbo\"  # \"gpt-4\"\n",
        "llm = ChatOpenAI(model_name=MODEL_NAME,temperature=0.0)\n",
        "compressor = LLMChainExtractor.from_llm(llm)\n",
        "compression_retriever = ContextualCompressionRetriever(base_compressor=compressor,\n",
        "                                                       base_retriever=retriever)\n",
        "\n",
        "question = \"What are generative agents?\"\n",
        "\n",
        "compressed_docs = compression_retriever.get_relevant_documents(question)\n",
        "pretty_print_docs(compressed_docs)\n",
        "\n",
        "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
        "result = llm_chain.predict(question=question, context=compressed_docs)\n",
        "print(\"-\" * 70)\n",
        "print(f\"Question: {question} \\nAnswer: {result}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Pinecone example code \n",
        "This code <font color=red>probably will not run for you</font>.  It depends on having a Pinecone account\n",
        "setup and usable via their API.</br>\n",
        "If you do have an account, then you should be able to modify the code to work. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#### this program does not query openai it merely gets embeddings from openai, and \n",
        "#### places them into a pinecone vector DB along with the passage for each embedding\n",
        "\n",
        "import os, re, time\n",
        "\n",
        "content = \"\"\n",
        "with open(\"txt/gettysburg.txt\") as f:\n",
        "    content = \"\"\n",
        "    lines = f.readlines()\n",
        "    for line in lines:\n",
        "        content += line.strip() + \" \"\n",
        "\n",
        "# --------\n",
        "\n",
        "import openai\n",
        "import pinecone  # pip install pinecone-client\n",
        "import pinecone.info\n",
        "import numpy as np\n",
        "\n",
        "EMBED_MODEL = \"text-embedding-ada-002\"\n",
        "\n",
        "# openai.organization = os.getenv(\"OPENAI_ORG\")\n",
        "with open(\"openaiorg.txt\") as f:\n",
        "    openai.organization = f.read().strip()\n",
        "\n",
        "# pinecone_api_key = os.getenv(\"PINECONE_API_KEY\")\n",
        "with open(\"pineconekey.txt\") as f:\n",
        "    pinecone_api_key = f.read().strip()\n",
        "pinecone.init(api_key=pinecone_api_key, environment='us-west1-gcp')\n",
        "version_info = pinecone.info.version()\n",
        "server_version = \".\".join(version_info.server.split(\".\")[:2])\n",
        "client_version = \".\".join(version_info.client.split(\".\")[:2])\n",
        "print(server_version,client_version)\n",
        "## assert client_version == server_version, \"Please upgrade pinecone-client.\"\n",
        "\n",
        "passages = []\n",
        "num_words_per_chunk = 100\n",
        "words = content.split()\n",
        "for i in range(0, len(words), num_words_per_chunk):\n",
        "    chunk = \" \".join(words[i:i+num_words_per_chunk])\n",
        "    passages.append(chunk)\n",
        "\n",
        "batch_size = 32\n",
        "embeddings_all = []\n",
        "embeds_as_arrays = []  # need list of arrays to create index\n",
        "print(\"NUM_PASSAGES\",len(passages),\"APPROX_NUM_BATCHES\",len(passages)//batch_size)\n",
        "for i in range(0, len(passages), batch_size):\n",
        "    batch = passages[i : i+batch_size]\n",
        "    res = openai.Embedding.create(input=batch, engine=EMBED_MODEL)\n",
        "    embeds = [record['embedding'] for record in res['data']]\n",
        "    embeddings_all.extend(embeds)\n",
        "print(len(passages),len(embeddings_all))\n",
        "\n",
        "dim = len(embeddings_all[0])\n",
        "index_name = \"gettysburg\"\n",
        "if index_name in pinecone.list_indexes():\n",
        "    print(\"DEL INDEX\")\n",
        "    pinecone.delete_index(index_name)\n",
        "    print(\"DEL DONE\")\n",
        "    # pass\n",
        "ctime = time.time()\n",
        "print(\"CREATE INDEX\")\n",
        "pinecone.create_index(name=index_name, dimension=dim, metric=\"cosine\")\n",
        "print(\"CREATE DONE\",time.time()-ctime)\n",
        "index = pinecone.Index(index_name=index_name)\n",
        "vecIDs = [ str(i) for i in range(len(embeddings_all)) ]   # ids should be str\n",
        "meta = [{'text': passage} for passage in passages]\n",
        "rc = index.upsert(vectors=zip(vecIDs, embeddings_all, meta))\n",
        "print(rc)\n",
        "print( index.describe_index_stats() )\n",
        "len_embeds = len(embeddings_all[0])\n",
        "\n",
        "stime = time.time()\n",
        "\n",
        "query = \"Which speech began with 'Four score and seven years ago'?\"\n",
        "res = openai.Embedding.create (\n",
        "    input=[query], engine=EMBED_MODEL\n",
        ")\n",
        "q_embed = res['data'][0]['embedding']\n",
        "###### q_embed = np.array(q_embed).reshape( (1,len(q_embed)) )\n",
        "###### print(\"QEMBED\",q_embed.shape)\n",
        "\n",
        "rc = index.query(\n",
        "        vector=q_embed,\n",
        "        top_k=1,  # just going for 1 in this tiny demo\n",
        "        include_metadata=True,\n",
        "        include_values=True)\n",
        "for x in rc[\"matches\"]:\n",
        "    print(x[\"id\"])\n",
        "    print(x[\"metadata\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Ensemble RAG example\n",
        "This code demos an ensemble of vectordb search with keyword search.  The keyword search is based on BM25 which is</br>\n",
        "used widely, e.g. by ElasticSearch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys, os, time\n",
        "\n",
        "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
        "from langchain.document_loaders import TextLoader, DirectoryLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "\n",
        "embedding = OpenAIEmbeddings()\n",
        "\n",
        "loader = DirectoryLoader(\"./\", glob=\"./txt/*.txt\", loader_cls=TextLoader)\n",
        "documents = loader.load()\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
        "chunks = text_splitter.split_documents(documents)\n",
        "texts = [ doc.page_content for doc in documents ]\n",
        "\n",
        "bm25_retriever = BM25Retriever.from_texts(texts)\n",
        "bm25_retriever.k = 2\n",
        "\n",
        "result_bm25 = bm25_retriever.get_relevant_documents(\"soviet missiles\")\n",
        "print(\"RESBM25\",len(result_bm25))\n",
        "for doc in result_bm25:\n",
        "    print(len(doc.page_content))\n",
        "print(\"-\" * 50)\n",
        "\n",
        "faiss_vectorstore = FAISS.from_texts(texts, embedding)\n",
        "faiss_retriever = faiss_vectorstore.as_retriever(search_kwargs={\"k\": 2})\n",
        "\n",
        "result_faiss = faiss_retriever.get_relevant_documents(\"soviet missiles\")\n",
        "print(\"RESFAISS\",len(result_faiss))\n",
        "for doc in result_faiss:\n",
        "    print(len(doc.page_content))\n",
        "print(\"-\" * 50)\n",
        "\n",
        "ensemble_retriever = EnsembleRetriever(retrievers=[bm25_retriever, faiss_retriever],\n",
        "                                       weights=[0.5, 0.5])\n",
        "\n",
        "ensemble_docs = ensemble_retriever.get_relevant_documents(\"soviet missiles\")\n",
        "print(\"RESENSEM\",len(ensemble_docs))\n",
        "for doc in ensemble_docs:\n",
        "    print(len(doc.page_content))\n",
        "print(\"-\" * 50)\n",
        "\n",
        "#### IMPORTANT NOTE ######## ****************\n",
        "####   If the total size of the page_content in the ensemble_docs is too large,\n",
        "####   you will get an error back from OpenAI because you have exceeded the token\n",
        "####   limit.  So, you may have to reduce it before using in the query.\n",
        "query = \"what was the significance of 'soviet missiles'?\"\n",
        "llm = ChatOpenAI(model_name=\"gpt-4\", temperature=0.7, max_tokens = 128,)\n",
        "chain = load_qa_chain(llm,chain_type=\"stuff\") # stuff all in at once\n",
        "response = chain.run(input_documents=ensemble_docs,question=query)\n",
        "print(response)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyMZi6NDm9npKydMJRyJPAT2",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
