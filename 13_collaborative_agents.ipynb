{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9IJvl-EEESCA"
      },
      "source": [
        "# <font color=red>LangChain:  Collaborative Agents to reason, e.g. to solve puzzles</font>\n",
        "- https://docs.langchain.com/docs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h4>\n",
        "We are attempting to create a simplified version of agent-based systems such as ChatDev, MetaGPT, Autogen, etc.</br>\n",
        "Our version will instantiate our own Agent class as needed, and the main will drive their interactions.\n",
        "</h4>\n",
        "<span style=\"font-family:'Comic Sans MS', cursive, sans-serif;\"><font color=orange>\n",
        "## Demo 1 - Example that uses 2 agents (solver and analyzer) to solve puzzle about using water jugs of various sizes.\n",
        "</font></span></br>\n",
        "The trick about this puzzle is that the LLM may want to use both the 12-liter and 6-liter jugs, but only the 6 is required.</br>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys, os, time\n",
        "\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "class Agent:\n",
        "    def __init__(self,name=\"agent\", role=\"\", llm=None, sys_msg=\"\"):\n",
        "        self.name = name\n",
        "        self.role = role\n",
        "        self.llm = llm\n",
        "        self.memory = ConversationBufferMemory(memory_key=\"chat_history\")\n",
        "        assert sys_msg, \"Each agent must be initialized with a system message.\"\n",
        "        post_sys_msg = \"\"\"\n",
        "            Previous conversation:\n",
        "            {chat_history}\n",
        "            New query: {question}\n",
        "            Response:\n",
        "        \"\"\"\n",
        "        post_sys_msg = \"\\n\".join([ l.strip() for l in post_sys_msg.split(\"\\n\") ])\n",
        "        template = sys_msg + post_sys_msg\n",
        "        self.prompt = PromptTemplate.from_template(template)\n",
        "        self.conversation = LLMChain(llm=self.llm, prompt=self.prompt,\n",
        "                                     memory=self.memory, verbose=False)\n",
        "\n",
        "    def run(self,query):\n",
        "        response = self.conversation({\"question\": query})\n",
        "        print(\"RESPONSE FROM\",self.name)\n",
        "        print(response[\"text\"])\n",
        "        print(\"-\" * 50)\n",
        "        return response[\"text\"]\n",
        "\n",
        "#### setup the solver agent\n",
        "solver_sys_msg = \"\"\"\n",
        "You are an excellent problem solver.\n",
        "You suggest solutions to problems that require as few steps as possible.\n",
        "You also may have an iterative conversation with analyzer1 who probes you with questions\n",
        "to help determine if there may be a solution with fewer steps.\n",
        "If you are responding to an analysis of a solution and you fully agree with analyzers's\n",
        "proposal but do not make any changes based on it, then say that and then print:  ALL_AGREE\n",
        "\"\"\"\n",
        "\n",
        "solver_llm = ChatOpenAI(model_name=\"gpt-4\", temperature=0.0, max_tokens=256)\n",
        "\n",
        "solver1 = Agent(\n",
        "    name=\"solver1\",\n",
        "    role=\"Propose solutions to small puzzles\",\n",
        "    llm=solver_llm,\n",
        "    sys_msg=solver_sys_msg,\n",
        ")\n",
        "\n",
        "\n",
        "#### setup the analyzer agent\n",
        "analyzer_sys_msg = \"\"\"\n",
        "You are an excellent problem solving helper.\n",
        "You do not suggest solutions yourself, but you question solver1 about any potential\n",
        "solutions that it has developed, probing for ways to improve its solution by reducing\n",
        "the number of steps.\n",
        "If you have a better idea, you can pose it as a question.\n",
        "If you do not want to suggest changes to solver1's proposed solution, say that and then\n",
        "print:  ALL_AGREE\n",
        "\"\"\"\n",
        "\n",
        "analyzer_llm = ChatOpenAI(model_name=\"gpt-4\", temperature=0.0, max_tokens=256)\n",
        "\n",
        "analyzer1 = Agent(\n",
        "    name=\"analyzer1\",\n",
        "    role=\"Analyze solutions from solver1\",\n",
        "    llm=analyzer_llm,\n",
        "    sys_msg=analyzer_sys_msg,\n",
        ")\n",
        "\n",
        "\n",
        "#### now do the solve-and-analyze loop\n",
        "user_query = \"\"\"\n",
        "If I have two empty jugs, a 12-liter jug and a 6-liter jug, what is a way to measure and\n",
        "pour exactly 6 liters into a barrel using the fewest possible number of steps?\n",
        "\"\"\"\n",
        "\n",
        "analyzer_response = user_query  # prime for loops\n",
        "NUM_ITERS = 3  # may put in sys.argv later\n",
        "for iteridx in range(NUM_ITERS):\n",
        "    solver_response = solver1.run(analyzer_response)\n",
        "    if not solver_response  or  \"ALL_AGREE\" in solver_response:\n",
        "        break\n",
        "    analyzer_response = analyzer1.run(solver_response)\n",
        "    if not analyzer_response  or  \"ALL_AGREE\" in analyzer_response:\n",
        "        break"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyMZi6NDm9npKydMJRyJPAT2",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
