{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9IJvl-EEESCA"
      },
      "source": [
        "# <font color=red>LangChain:  Llama-2</font>\n",
        "- https://docs.langchain.com/docs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<span style=\"font-family:'Comic Sans MS', cursive, sans-serif;\"><font color=orange>\n",
        "## Llama-2\n",
        "</font></span>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### This demo shows how to use LangChain with Llama-2, which is currently somewhat different from using it with OpenAI models, because LangChain does not have built-in support for Llama-2 at the moment.\n",
        "The demo assumes you have a local version of Llama-2-7B-chat-hf installed. </br>\n",
        "It uses a file named 83332.12.txt which contains genomic data.</br>\n",
        "It places the data into a ChromaDB vector database and uses the database in conjunction with Llama-2 to answer queries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.vectorstores import Chroma\n",
        "## from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.document_loaders import DirectoryLoader\n",
        "from langchain import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "\n",
        "# for llama-2\n",
        "import transformers\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "\n",
        "loader = TextLoader(\"./83332.12.txt\")   ### just the one file\n",
        "\n",
        "documents = loader.load()\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    separators = [\"\\n\"],\n",
        "    keep_separator = False,\n",
        "    chunk_size = 0,    # just splits on lines (separators)\n",
        "    chunk_overlap  = 0,\n",
        "    length_function = len,\n",
        "    is_separator_regex = False,\n",
        "    # add_start_index = True,\n",
        ")\n",
        "texts = text_splitter.split_documents(documents)\n",
        "\n",
        "## embeddings = OpenAIEmbeddings()\n",
        "model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
        "model_kwargs = {\"device\": \"cuda\"}\n",
        "embeddings = HuggingFaceEmbeddings(model_name=model_name, model_kwargs=model_kwargs)\n",
        "\n",
        "vectordb = Chroma.from_documents(documents=texts, embedding=embeddings,) # no persist\n",
        "\n",
        "retriever = vectordb.as_retriever(search_kwargs={\"k\": 2}) # k override 4 with 2\n",
        "\n",
        "###  now setup the LLM  ###\n",
        "# llm = ChatOpenAI(model_name=\"gpt-4\", temperature=0.0, max_tokens = 128,)\n",
        "\n",
        "model_id = \"./Llama-2-7b-chat-hf\"   # first, setup the model\n",
        "model_config = transformers.AutoConfig.from_pretrained(model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    config=model_config,\n",
        "    device_map='auto',\n",
        "    # trust_remote_code=True,  # if using at huggingface\n",
        "    # use_auth_token=hf_key,   # if using at huggingface\n",
        ")\n",
        "model.eval()\n",
        "print(\"DEVICE\",next(model.parameters()).device)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)  # second, setup the tokenizer\n",
        "\n",
        "pipe = pipeline(            # third, setup the pipeline using the model and tokenizer\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=128,\n",
        "    temperature=0.3,\n",
        "    repetition_penalty=1.1,\n",
        "    return_full_text=True,\n",
        "    device_map='auto'\n",
        ")\n",
        "\n",
        "llm = HuggingFacePipeline(pipeline=pipe)   # fourth / last, setup the LLM\n",
        "\n",
        "qa_chain = RetrievalQA.from_chain_type(llm=llm,\n",
        "                                       chain_type=\"stuff\", # stuff all in at once\n",
        "                                       retriever=retriever,\n",
        "                                       # return_source_documents=True) # we know :-)\n",
        "                                      )\n",
        "\n",
        "query = \"Which genome has ID 83332.12 ?\"  # Mycobacterium tuberculosis H37Rv\n",
        "\n",
        "llm_response = qa_chain(query)\n",
        "print(\"RESPONSE\")\n",
        "print(llm_response['result'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Chatbot for llama-2-7B-chat\n",
        "This chatbot demos using either of two kinds of memory depending on which one is commented out/in:</br>\n",
        "<pre>\n",
        "    memory = ConversationBufferMemory(memory_key=\"chat_history\")\n",
        "    # memory = ConversationSummaryMemory(llm=llm, memory_key=\"chat_history\")\n",
        "</pre>\n",
        "And, it has been tested with two versions llama-2 (plain and fine-tuned):</br>\n",
        "<pre>\n",
        "    model_id = \"./Llama-2-7b-chat-hf\"\n",
        "    # model_id = \"./bio_FT_Llama-2-7b-chat-hf\"    # fine-tuned model\n",
        "</pre>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "import sys, os\n",
        "\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.memory import ConversationBufferMemory      # you should probably choose\n",
        "from langchain.memory import ConversationSummaryMemory     #   just one of these\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "\n",
        "import torch\n",
        "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
        "import transformers\n",
        "\n",
        "model_id = \"./Llama-2-7b-chat-hf\"\n",
        "# model_id = \"./bio_FT_Llama-2-7b-chat-hf\"    # fine-tuned model\n",
        "\n",
        "tokenizer = LlamaTokenizer.from_pretrained(model_id)\n",
        "\n",
        "model = LlamaForCausalLM.from_pretrained(\n",
        "    model_id, load_in_8bit=True, device_map='auto', torch_dtype=torch.float16,\n",
        ")\n",
        "model.eval()\n",
        "\n",
        "generate_text = transformers.pipeline(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    return_full_text=True,  # langchain expects the full text\n",
        "    task='text-generation',\n",
        "    # we pass model parameters here too\n",
        "    # stopping_criteria=stopping_criteria,  # without this model rambles during chat\n",
        "    temperature=0.1,  # 'randomness' of outputs, 0.0 is the min and 1.0 the max\n",
        "    max_new_tokens=512,  # mex number of tokens to generate in the output\n",
        "    repetition_penalty=1.1  # without this output begins repeating\n",
        ")\n",
        "\n",
        "llm = HuggingFacePipeline(pipeline=generate_text)\n",
        "\n",
        "# \"chat_history\" in the prompt template will be set by retrieving values from memory\n",
        "template = \"\"\"\n",
        "You are a chatbot that can answer general knowledge questions.\n",
        "However, you are very well educated about genomics and are prepared to give\n",
        "insightful answers on that topic.\n",
        "Previous conversation:\n",
        "{chat_history}\n",
        "New question: {question}\n",
        "Response:\n",
        "\"\"\"\n",
        "\n",
        "prompt = PromptTemplate.from_template(template)\n",
        "\n",
        "memory = ConversationBufferMemory(memory_key=\"chat_history\")\n",
        "# memory = ConversationSummaryMemory(llm=llm, memory_key=\"chat_history\")\n",
        "\n",
        "conversation = LLMChain(llm=llm, prompt=prompt, verbose=False, memory=memory)\n",
        "\n",
        "while True:\n",
        "    print(\"query: \", end=\"\", flush=True)\n",
        "    query = sys.stdin.readline()\n",
        "    if not query: break\n",
        "    response = conversation({\"question\": query})\n",
        "    text_response = response[\"text\"]\n",
        "    print(f\"    {text_response}\\n\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyMZi6NDm9npKydMJRyJPAT2",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
