{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9IJvl-EEESCA"
      },
      "source": [
        "# <font color=red>LangChain:  Llama-2</font>\n",
        "- https://docs.langchain.com/docs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<span style=\"font-family:'Comic Sans MS', cursive, sans-serif;\"><font color=orange>\n",
        "## Llama-2\n",
        "</font></span>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### This demo shows how to use LangChain with Llama-2, which is currently somewhat different from using it with OpenAI models, because LangChain does not have built-in support for Llama-2 at the moment.\n",
        "The demo assumes you have a local version of Llama-2-7B-chat-hf installed. </br>\n",
        "It uses a file named 83332.12.txt which contains genomic data.</br>\n",
        "It places the data into a ChromaDB vector database and uses the database in conjunction with Llama-2 to answer queries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.vectorstores import Chroma\n",
        "## from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.document_loaders import DirectoryLoader\n",
        "from langchain import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "\n",
        "# for llama-2\n",
        "import transformers\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "\n",
        "loader = TextLoader(\"./83332.12.txt\")   ### just the one file\n",
        "\n",
        "documents = loader.load()\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    separators = [\"\\n\"],\n",
        "    keep_separator = False,\n",
        "    chunk_size = 0,    # just splits on lines (separators)\n",
        "    chunk_overlap  = 0,\n",
        "    length_function = len,\n",
        "    is_separator_regex = False,\n",
        "    # add_start_index = True,\n",
        ")\n",
        "texts = text_splitter.split_documents(documents)\n",
        "\n",
        "## embeddings = OpenAIEmbeddings()\n",
        "model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
        "model_kwargs = {\"device\": \"cuda\"}\n",
        "embeddings = HuggingFaceEmbeddings(model_name=model_name, model_kwargs=model_kwargs)\n",
        "\n",
        "vectordb = Chroma.from_documents(documents=texts, embedding=embeddings,) # no persist\n",
        "\n",
        "retriever = vectordb.as_retriever(search_kwargs={\"k\": 2}) # k override 4 with 2\n",
        "\n",
        "###  now setup the LLM  ###\n",
        "# llm = ChatOpenAI(model_name=\"gpt-4\", temperature=0.0, max_tokens = 128,)\n",
        "\n",
        "model_id = \"./Llama-2-7b-chat-hf\"   # first, setup the model\n",
        "model_config = transformers.AutoConfig.from_pretrained(model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    config=model_config,\n",
        "    device_map='auto',\n",
        "    # trust_remote_code=True,  # if using at huggingface\n",
        "    # use_auth_token=hf_key,   # if using at huggingface\n",
        ")\n",
        "model.eval()\n",
        "print(\"DEVICE\",next(model.parameters()).device)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)  # second, setup the tokenizer\n",
        "\n",
        "pipe = pipeline(            # third, setup the pipeline using the model and tokenizer\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=128,\n",
        "    temperature=0.3,\n",
        "    repetition_penalty=1.1,\n",
        "    return_full_text=True,\n",
        "    device_map='auto'\n",
        ")\n",
        "\n",
        "llm = HuggingFacePipeline(pipeline=pipe)   # fourth / last, setup the LLM\n",
        "\n",
        "qa_chain = RetrievalQA.from_chain_type(llm=llm,\n",
        "                                       chain_type=\"stuff\", # stuff all in at once\n",
        "                                       retriever=retriever,\n",
        "                                       # return_source_documents=True) # we know :-)\n",
        "                                      )\n",
        "\n",
        "query = \"Which genome has ID 83332.12 ?\"  # Mycobacterium tuberculosis H37Rv\n",
        "\n",
        "llm_response = qa_chain(query)\n",
        "print(\"RESPONSE\")\n",
        "print(llm_response['result'])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyMZi6NDm9npKydMJRyJPAT2",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
